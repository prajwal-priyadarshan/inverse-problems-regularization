{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7847c9be",
   "metadata": {},
   "source": [
    "# 7. Complete Comparison: All Reconstruction Methods\n",
    "\n",
    "## Objective\n",
    "Compare **all 5 reconstruction methods** on the same toy problem:\n",
    "1. **Pseudoinverse** (fails on noisy problems)\n",
    "2. **Tikhonov Regularization** (classical L2 regularization)\n",
    "3. **TSVD** (Truncated SVD)\n",
    "4. **NSIT** (Non-Stationary Iterated Tikhonov)\n",
    "5. **FNSIT** (Fast NSIT - approximate inner solve)\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Inverse Problem Setup\n",
    "Given:\n",
    "- Forward model: $y = Ax + \\eta$ where $\\eta \\sim \\mathcal{N}(0, \\delta^2 I)$\n",
    "- Measurement: $y \\in \\mathbb{R}^m$\n",
    "- Unknown signal: $x \\in \\mathbb{R}^n$\n",
    "- Operator: $A \\in \\mathbb{R}^{m \\times n}$ (ill-conditioned or rank-deficient)\n",
    "\n",
    "Goal: Reconstruct $x$ from noisy measurement $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6143ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c22e15",
   "metadata": {},
   "source": [
    "## Step 1: Generate Toy Problem - Signal and Blur Operator\n",
    "\n",
    "### True Signal\n",
    "We create a piecewise signal with:\n",
    "- Smooth sinusoidal component\n",
    "- Sharp jump discontinuity\n",
    "- Narrow spike (high-frequency feature)\n",
    "\n",
    "$$x_{\\text{true}}(t) = 0.6\\sin(6\\pi t) + 0.6 \\cdot \\mathbb{1}_{[0.35, 0.55]}(t) + 0.9\\exp\\left(-\\frac{(t-0.75)^2}{0.004}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_signal(n=100):\n",
    "    \"\"\"\n",
    "    Generate a challenging test signal with multiple features:\n",
    "    - Smooth oscillations (low frequency)\n",
    "    - Sharp jump (discontinuity)\n",
    "    - Narrow spike (very high frequency)\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 1, n)\n",
    "    \n",
    "    # Component 1: Smooth sinusoid\n",
    "    component1 = 0.6 * np.sin(2 * np.pi * 3 * t)\n",
    "    \n",
    "    # Component 2: Jump discontinuity (indicator function)\n",
    "    component2 = 0.6 * ((t >= 0.35) & (t <= 0.55)).astype(float)\n",
    "    \n",
    "    # Component 3: Sharp Gaussian spike\n",
    "    component3 = 0.9 * np.exp(-((t - 0.75)**2) / (2 * 0.002))\n",
    "    \n",
    "    x_true = component1 + component2 + component3\n",
    "    \n",
    "    return t, x_true\n",
    "\n",
    "\n",
    "# Generate signal\n",
    "n = 100  # Signal dimension\n",
    "t, x_true = generate_true_signal(n)\n",
    "\n",
    "print(f\"Signal dimension: n = {n}\")\n",
    "print(f\"Signal range: [{x_true.min():.3f}, {x_true.max():.3f}]\")\n",
    "print(f\"Signal L2 norm: {np.linalg.norm(x_true):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b412af7",
   "metadata": {},
   "source": [
    "### Forward Operator: Gaussian Blur\n",
    "\n",
    "Create a **blur matrix** $A$ from a Gaussian kernel:\n",
    "\n",
    "$$K(i) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{i^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The blur matrix $A$ is a **Toeplitz matrix** where:\n",
    "$$A_{ij} = K(j - i)$$\n",
    "\n",
    "This creates an **ill-conditioned** linear operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_kernel(size, sigma):\n",
    "    \"\"\"\n",
    "    Create 1D Gaussian kernel.\n",
    "    \n",
    "    Math: K(x) = (1/âˆš(2Ï€ÏƒÂ²)) * exp(-xÂ²/(2ÏƒÂ²))\n",
    "    \"\"\"\n",
    "    assert size % 2 == 1, \"Kernel size must be odd\"\n",
    "    \n",
    "    radius = size // 2\n",
    "    x = np.arange(-radius, radius + 1)\n",
    "    \n",
    "    # Gaussian formula\n",
    "    kernel = np.exp(-(x**2) / (2 * sigma**2))\n",
    "    \n",
    "    # Normalize to sum to 1 (preserve signal energy)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "\n",
    "def create_blur_matrix(n, sigma=2.5, kernel_size=21):\n",
    "    \"\"\"\n",
    "    Create blur operator A as a Toeplitz matrix.\n",
    "    \n",
    "    Each row applies the Gaussian kernel centered at that position.\n",
    "    Uses zero-padding at boundaries.\n",
    "    \"\"\"\n",
    "    kernel = create_gaussian_kernel(kernel_size, sigma)\n",
    "    radius = kernel_size // 2\n",
    "    \n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Distance between positions\n",
    "            offset = j - i + radius\n",
    "            \n",
    "            # Check if within kernel support\n",
    "            if 0 <= offset < kernel_size:\n",
    "                A[i, j] = kernel[offset]\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "# Create blur operator\n",
    "sigma_blur = 2.5\n",
    "kernel_size = 21\n",
    "A = create_blur_matrix(n, sigma=sigma_blur, kernel_size=kernel_size)\n",
    "\n",
    "print(f\"\\nBlur operator A: {A.shape}\")\n",
    "print(f\"Kernel size: {kernel_size}, Ïƒ = {sigma_blur}\")\n",
    "print(f\"Matrix properties:\")\n",
    "print(f\"  - Symmetric: {np.allclose(A, A.T)}\")\n",
    "print(f\"  - Sum of each row: {A.sum(axis=1).mean():.4f} (should â‰ˆ 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27497379",
   "metadata": {},
   "source": [
    "### Problem Diagnostics: SVD Analysis\n",
    "\n",
    "Perform **Singular Value Decomposition**:\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "Key quantities:\n",
    "- **Condition number**: $\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$\n",
    "- **Effective rank**: Number of singular values above threshold\n",
    "- **Singular value decay**: Indicates ill-posedness severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd_detailed(A):\n",
    "    \"\"\"\n",
    "    Compute full SVD and diagnostics.\n",
    "    \n",
    "    Returns: U, s (singular values), VT, and diagnostics dict\n",
    "    \"\"\"\n",
    "    U, s, VT = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    diagnostics = {\n",
    "        'condition_number': s[0] / s[-1] if s[-1] > 0 else np.inf,\n",
    "        'max_singular_value': s[0],\n",
    "        'min_singular_value': s[-1],\n",
    "        'effective_rank_1e-10': np.sum(s > s[0] * 1e-10),\n",
    "        'effective_rank_1e-6': np.sum(s > s[0] * 1e-6),\n",
    "        'n_singular_values': len(s)\n",
    "    }\n",
    "    \n",
    "    return U, s, VT, diagnostics\n",
    "\n",
    "\n",
    "# Analyze operator A\n",
    "U, singular_values, VT, diagnostics = compute_svd_detailed(A)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVD DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Condition number Îº(A): {diagnostics['condition_number']:.2e}\")\n",
    "print(f\"Singular value range: [{diagnostics['min_singular_value']:.2e}, {diagnostics['max_singular_value']:.2e}]\")\n",
    "print(f\"Effective rank (tol=10â»Â¹â°): {diagnostics['effective_rank_1e-10']}\")\n",
    "print(f\"Effective rank (tol=10â»â¶): {diagnostics['effective_rank_1e-6']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226000a",
   "metadata": {},
   "source": [
    "### Generate Measurements: Clean and Noisy\n",
    "\n",
    "$$y_{\\text{clean}} = Ax_{\\text{true}}$$\n",
    "$$y_{\\text{noisy}} = y_{\\text{clean}} + \\eta, \\quad \\eta \\sim \\mathcal{N}(0, \\delta^2\\|y_{\\text{clean}}\\|^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise_relative(y_clean, noise_level):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise with relative noise level.\n",
    "    \n",
    "    noise_level: relative noise (e.g., 0.01 = 1%)\n",
    "    \n",
    "    Math:\n",
    "        Ïƒ = noise_level * ||y_clean||\n",
    "        y_noisy = y_clean + N(0, ÏƒÂ²I)\n",
    "    \"\"\"\n",
    "    signal_norm = np.linalg.norm(y_clean)\n",
    "    noise_std = noise_level * signal_norm\n",
    "    \n",
    "    noise = noise_std * np.random.randn(len(y_clean))\n",
    "    y_noisy = y_clean + noise\n",
    "    \n",
    "    return y_noisy, noise_std\n",
    "\n",
    "\n",
    "# Forward operator application\n",
    "y_clean = A @ x_true\n",
    "\n",
    "# Add noise\n",
    "noise_level = 0.01  # 1% noise\n",
    "y_noisy, noise_std = add_gaussian_noise_relative(y_clean, noise_level)\n",
    "\n",
    "print(f\"\\nMeasurement generation:\")\n",
    "print(f\"  Clean data norm: {np.linalg.norm(y_clean):.4f}\")\n",
    "print(f\"  Noise level: {noise_level * 100:.1f}%\")\n",
    "print(f\"  Noise standard deviation: {noise_std:.4e}\")\n",
    "print(f\"  SNR: {20 * np.log10(np.linalg.norm(y_clean) / np.linalg.norm(y_noisy - y_clean)):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36764d71",
   "metadata": {},
   "source": [
    "### Visualize Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot 1: True signal\n",
    "axes[0].plot(t, x_true, 'k-', linewidth=2.5, label='True Signal')\n",
    "axes[0].fill_between(t, 0, x_true, alpha=0.2, color='blue')\n",
    "axes[0].set_title('True Signal x(t)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('t')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Measurements\n",
    "axes[1].plot(t, y_clean, 'b-', linewidth=2, label='Clean y = Ax', alpha=0.7)\n",
    "axes[1].plot(t, y_noisy, 'r--', linewidth=1.5, label=f'Noisy (Î´={noise_level*100:.1f}%)', alpha=0.8)\n",
    "axes[1].set_title('Forward Operator: y = Ax + Î·', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('t')\n",
    "axes[1].set_ylabel('Measurement')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Singular values (log scale)\n",
    "axes[2].semilogy(range(1, len(singular_values)+1), singular_values, 'bo-', linewidth=2, markersize=5)\n",
    "axes[2].axhline(singular_values[0]*1e-10, color='r', linestyle='--', linewidth=1.5, label='Tolerance 10â»Â¹â°')\n",
    "axes[2].set_title(f'Singular Values (Îº={diagnostics[\"condition_number\"]:.1e})', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Index i')\n",
    "axes[2].set_ylabel('Ïƒáµ¢')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Problem setup visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818832df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Method Implementations - From Scratch\n",
    "\n",
    "We implement all 5 methods with **explicit mathematical formulations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dfe5a",
   "metadata": {},
   "source": [
    "### Method 1: Pseudoinverse (Baseline - Fails on Noisy Data)\n",
    "\n",
    "**Direct inversion** using SVD:\n",
    "\n",
    "$$x_{\\text{pinv}} = A^\\dagger y = V\\Sigma^{-1}U^T y$$\n",
    "\n",
    "Where $\\Sigma^{-1}_{ii} = 1/\\sigma_i$ (amplifies small singular values â†’ amplifies noise!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828839d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudoinverse_reconstruct(A, y):\n",
    "    \"\"\"\n",
    "    Pseudoinverse reconstruction: x = Aâ€  y\n",
    "    \n",
    "    Math:\n",
    "        A = U Î£ V^T\n",
    "        Aâ€  = V Î£^(-1) U^T\n",
    "        x = V Î£^(-1) U^T y\n",
    "    \n",
    "    Problem: Amplifies noise through small singular values!\n",
    "    \"\"\"\n",
    "    U, s, VT = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Compute Aâ€  y = V Î£^(-1) U^T y\n",
    "    # Step 1: U^T y\n",
    "    UTy = U.T @ y\n",
    "    \n",
    "    # Step 2: Î£^(-1) (U^T y) - this is where noise amplification happens!\n",
    "    s_inv = 1.0 / s  # âš ï¸ Small singular values become huge!\n",
    "    scaled = s_inv * UTy\n",
    "    \n",
    "    # Step 3: V (Î£^(-1) U^T y)\n",
    "    x_pinv = VT.T @ scaled\n",
    "    \n",
    "    return x_pinv\n",
    "\n",
    "\n",
    "print(\"âœ“ Pseudoinverse implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572e738",
   "metadata": {},
   "source": [
    "### Method 2: Tikhonov Regularization (L2 Penalty)\n",
    "\n",
    "**Regularized least squares**:\n",
    "\n",
    "$$x_{\\lambda} = \\arg\\min_x \\|Ax - y\\|^2 + \\lambda\\|x\\|^2$$\n",
    "\n",
    "**Closed-form solution**:\n",
    "$$x_{\\lambda} = (A^TA + \\lambda I)^{-1}A^T y$$\n",
    "\n",
    "**SVD form** (more stable):\n",
    "$$x_{\\lambda} = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\langle u_i, y\\rangle v_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tikhonov_reconstruct(A, y, lambda_reg):\n",
    "    \"\"\"\n",
    "    Tikhonov regularization: min ||Ax - y||Â² + Î»||x||Â²\n",
    "    \n",
    "    Solution: x = (A^T A + Î»I)^(-1) A^T y\n",
    "    \n",
    "    Using SVD form for numerical stability:\n",
    "        x = Î£áµ¢ [Ïƒáµ¢/(Ïƒáµ¢Â² + Î»)] * âŸ¨uáµ¢, yâŸ© * váµ¢\n",
    "    \n",
    "    Filter factor: f(Ïƒ) = Ïƒ/(ÏƒÂ² + Î»)\n",
    "    - Large Ïƒ: f â‰ˆ 1/Ïƒ (trusts data)\n",
    "    - Small Ïƒ: f â‰ˆ 0 (suppresses noise)\n",
    "    \"\"\"\n",
    "    U, s, VT = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Compute filter factors\n",
    "    filter_factors = s / (s**2 + lambda_reg)\n",
    "    \n",
    "    # Reconstruction\n",
    "    UTy = U.T @ y\n",
    "    scaled = filter_factors * UTy\n",
    "    x_tik = VT.T @ scaled\n",
    "    \n",
    "    return x_tik\n",
    "\n",
    "\n",
    "def tikhonov_optimal_parameter(A, y, x_true, lambda_range):\n",
    "    \"\"\"\n",
    "    Find optimal Î» by parameter sweep (oracle - requires true solution).\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for lam in lambda_range:\n",
    "        x_rec = tikhonov_reconstruct(A, y, lam)\n",
    "        error = np.linalg.norm(x_rec - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(error)\n",
    "    \n",
    "    optimal_idx = np.argmin(errors)\n",
    "    return lambda_range[optimal_idx], errors[optimal_idx]\n",
    "\n",
    "\n",
    "print(\"âœ“ Tikhonov regularization implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df961535",
   "metadata": {},
   "source": [
    "### Method 3: Truncated SVD (TSVD)\n",
    "\n",
    "**Spectral truncation**: Keep only $k$ largest singular values\n",
    "\n",
    "$$x_k = \\sum_{i=1}^k \\frac{\\langle u_i, y\\rangle}{\\sigma_i} v_i$$\n",
    "\n",
    "**Filter interpretation**:\n",
    "$$f_i = \\begin{cases}\n",
    "1/\\sigma_i & i \\leq k \\\\\n",
    "0 & i > k\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d717600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsvd_reconstruct(A, y, k):\n",
    "    \"\"\"\n",
    "    Truncated SVD: Use only k largest singular values.\n",
    "    \n",
    "    Math:\n",
    "        x_k = Î£áµ¢â‚Œâ‚áµ (âŸ¨uáµ¢, yâŸ©/Ïƒáµ¢) * váµ¢\n",
    "    \n",
    "    Filter:\n",
    "        f(Ïƒáµ¢) = 1/Ïƒáµ¢  if i â‰¤ k\n",
    "        f(Ïƒáµ¢) = 0     if i > k\n",
    "    \"\"\"\n",
    "    U, s, VT = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Truncate: keep only first k components\n",
    "    U_k = U[:, :k]\n",
    "    s_k = s[:k]\n",
    "    VT_k = VT[:k, :]\n",
    "    \n",
    "    # Reconstruction\n",
    "    UTy = U_k.T @ y\n",
    "    scaled = UTy / s_k\n",
    "    x_tsvd = VT_k.T @ scaled\n",
    "    \n",
    "    return x_tsvd\n",
    "\n",
    "\n",
    "def tsvd_optimal_parameter(A, y, x_true, k_range):\n",
    "    \"\"\"\n",
    "    Find optimal truncation k (oracle).\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for k in k_range:\n",
    "        x_rec = tsvd_reconstruct(A, y, k)\n",
    "        error = np.linalg.norm(x_rec - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(error)\n",
    "    \n",
    "    optimal_idx = np.argmin(errors)\n",
    "    return k_range[optimal_idx], errors[optimal_idx]\n",
    "\n",
    "\n",
    "print(\"âœ“ TSVD implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e85c16",
   "metadata": {},
   "source": [
    "### Method 4: NSIT (Non-Stationary Iterated Tikhonov)\n",
    "\n",
    "**Iterative method** with **decreasing regularization**:\n",
    "\n",
    "$$x_{n+1} = x_n + (A^TA + \\alpha_n I)^{-1}A^T(y - Ax_n)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_n = \\alpha_0 q^n$ (geometric decay, $0 < q < 1$)\n",
    "- Early iterations: Large $\\alpha_n$ â†’ strong regularization (suppress noise)\n",
    "- Late iterations: Small $\\alpha_n$ â†’ weak regularization (recover details)\n",
    "\n",
    "**Morozov discrepancy principle** (automatic stopping):\n",
    "$$\\|Ax_n - y\\| \\leq \\tau \\cdot \\delta \\|y\\|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95025f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nsit_reconstruct(A, y, alpha0, q, max_iter, tau, delta, x_true=None):\n",
    "    \"\"\"\n",
    "    NSIT with Morozov stopping.\n",
    "    \n",
    "    Iteration:\n",
    "        r_n = y - A x_n              (residual)\n",
    "        z_n = (A^T A + Î±â‚™I)^(-1) A^T r_n   (regularized update)\n",
    "        x_{n+1} = x_n + z_n\n",
    "        Î±â‚™ = Î±â‚€ * q^n                (decreasing schedule)\n",
    "    \n",
    "    Stopping: ||A x_n - y|| â‰¤ Ï„ * Î´ * ||y||\n",
    "    \"\"\"\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    # Precompute ATA\n",
    "    ATA = A.T @ A\n",
    "    \n",
    "    # Stopping threshold\n",
    "    y_norm = np.linalg.norm(y)\n",
    "    threshold = tau * delta * y_norm\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'residuals': [],\n",
    "        'errors': [],\n",
    "        'alphas': [],\n",
    "        'iterations': []\n",
    "    }\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Current regularization parameter\n",
    "        alpha_n = alpha0 * (q ** iteration)\n",
    "        \n",
    "        # Compute residual\n",
    "        residual = y - A @ x\n",
    "        residual_norm = np.linalg.norm(residual)\n",
    "        \n",
    "        # Morozov stopping criterion\n",
    "        if residual_norm <= threshold:\n",
    "            print(f\"  Morozov stopping at iteration {iteration}\")\n",
    "            break\n",
    "        \n",
    "        # Solve: (A^T A + Î±_n I) z = A^T r\n",
    "        rhs = A.T @ residual\n",
    "        regularized_matrix = ATA + alpha_n * np.eye(n)\n",
    "        z = np.linalg.solve(regularized_matrix, rhs)\n",
    "        \n",
    "        # Update\n",
    "        x = x + z\n",
    "        \n",
    "        # Track progress\n",
    "        history['residuals'].append(residual_norm)\n",
    "        history['alphas'].append(alpha_n)\n",
    "        history['iterations'].append(iteration)\n",
    "        \n",
    "        if x_true is not None:\n",
    "            error = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "            history['errors'].append(error)\n",
    "    \n",
    "    history['final_iteration'] = iteration\n",
    "    return x, history\n",
    "\n",
    "\n",
    "print(\"âœ“ NSIT implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ae74d",
   "metadata": {},
   "source": [
    "### Method 5: FNSIT (Fast NSIT - Approximate Inner Solve)\n",
    "\n",
    "**Key innovation**: Replace exact linear solve with **fast approximate solver**\n",
    "\n",
    "Instead of solving $(A^TA + \\alpha_n I)z = A^Tr$ exactly, use **gradient descent**:\n",
    "\n",
    "$$z^{(k+1)} = z^{(k)} - \\beta \\nabla f(z^{(k)})$$\n",
    "$$z^{(k+1)} = z^{(k)} - \\beta [(A^TA + \\alpha_n I)z^{(k)} - A^Tr]$$\n",
    "\n",
    "**Advantages**:\n",
    "- **Computational**: $O(mn \\cdot k_{\\text{inner}})$ vs $O(n^3)$ for exact solve\n",
    "- **Memory**: No need to form/store $A^TA$\n",
    "- **Implicit regularization**: Early stopping in inner solver adds regularization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnsit_fast_solver(ATA, rhs, alpha, inner_steps=5, step_size=0.1):\n",
    "    \"\"\"\n",
    "    Fast approximate solver for: (A^T A + Î± I) z = rhs\n",
    "    \n",
    "    Using gradient descent:\n",
    "        Minimize: f(z) = (1/2)||Az||Â² + (Î±/2)||z||Â² - âŸ¨rhs, zâŸ©\n",
    "        Gradient: âˆ‡f(z) = (A^T A + Î± I)z - rhs\n",
    "        Update: z â† z - Î² * âˆ‡f(z)\n",
    "    \n",
    "    Key: Few iterations (e.g., 3-10) provide good-enough approximation!\n",
    "    \"\"\"\n",
    "    n = len(rhs)\n",
    "    z = np.zeros(n)  # Cold start\n",
    "    \n",
    "    for _ in range(inner_steps):\n",
    "        # Gradient = (A^T A + Î± I)z - rhs\n",
    "        gradient = (ATA + alpha * np.eye(n)) @ z - rhs\n",
    "        \n",
    "        # Gradient descent step\n",
    "        z = z - step_size * gradient\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "def fnsit_reconstruct(A, y, alpha0, q, max_iter, tau, delta, \n",
    "                     inner_steps=5, step_size=0.1, x_true=None):\n",
    "    \"\"\"\n",
    "    Fast NSIT: Uses approximate fast solver instead of exact solve.\n",
    "    \n",
    "    Same outer iteration as NSIT, but:\n",
    "        - Replace exact solve with gradient descent\n",
    "        - Much faster per iteration\n",
    "        - Additional implicit regularization from approximate solve\n",
    "    \"\"\"\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    # Precompute A^T A (same as NSIT)\n",
    "    ATA = A.T @ A\n",
    "    \n",
    "    # Stopping threshold\n",
    "    y_norm = np.linalg.norm(y)\n",
    "    threshold = tau * delta * y_norm\n",
    "    \n",
    "    # History\n",
    "    history = {\n",
    "        'residuals': [],\n",
    "        'errors': [],\n",
    "        'alphas': [],\n",
    "        'iterations': []\n",
    "    }\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Current regularization\n",
    "        alpha_n = alpha0 * (q ** iteration)\n",
    "        \n",
    "        # Residual\n",
    "        residual = y - A @ x\n",
    "        residual_norm = np.linalg.norm(residual)\n",
    "        \n",
    "        # Morozov stopping\n",
    "        if residual_norm <= threshold:\n",
    "            print(f\"  FNSIT Morozov stopping at iteration {iteration}\")\n",
    "            break\n",
    "        \n",
    "        # Fast approximate solve (key difference!)\n",
    "        rhs = A.T @ residual\n",
    "        z = fnsit_fast_solver(ATA, rhs, alpha_n, inner_steps, step_size)\n",
    "        \n",
    "        # Update\n",
    "        x = x + z\n",
    "        \n",
    "        # Track\n",
    "        history['residuals'].append(residual_norm)\n",
    "        history['alphas'].append(alpha_n)\n",
    "        history['iterations'].append(iteration)\n",
    "        \n",
    "        if x_true is not None:\n",
    "            error = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "            history['errors'].append(error)\n",
    "    \n",
    "    history['final_iteration'] = iteration\n",
    "    return x, history\n",
    "\n",
    "\n",
    "print(\"âœ“ FNSIT implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78122f61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Run All Methods and Compare\n",
    "\n",
    "We now apply all 5 methods to the same noisy measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING ALL RECONSTRUCTION METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Container for results\n",
    "results = []\n",
    "\n",
    "# Method 1: Pseudoinverse\n",
    "print(\"\\n1. Pseudoinverse...\")\n",
    "x_pinv = pseudoinverse_reconstruct(A, y_noisy)\n",
    "error_pinv = np.linalg.norm(x_pinv - x_true) / np.linalg.norm(x_true)\n",
    "print(f\"   Error: {error_pinv:.6f}\")\n",
    "results.append({\n",
    "    'method': 'Pseudoinverse',\n",
    "    'x_rec': x_pinv,\n",
    "    'error': error_pinv,\n",
    "    'parameter': 'N/A',\n",
    "    'notes': 'Baseline (no regularization)'\n",
    "})\n",
    "\n",
    "# Method 2: Tikhonov\n",
    "print(\"\\n2. Tikhonov (oracle parameter sweep)...\")\n",
    "lambda_range = np.logspace(-8, 1, 200)\n",
    "lambda_opt, error_tik = tikhonov_optimal_parameter(A, y_noisy, x_true, lambda_range)\n",
    "x_tik = tikhonov_reconstruct(A, y_noisy, lambda_opt)\n",
    "print(f\"   Optimal Î» = {lambda_opt:.2e}\")\n",
    "print(f\"   Error: {error_tik:.6f}\")\n",
    "results.append({\n",
    "    'method': 'Tikhonov',\n",
    "    'x_rec': x_tik,\n",
    "    'error': error_tik,\n",
    "    'parameter': f'Î»={lambda_opt:.2e}',\n",
    "    'notes': 'Oracle optimal Î»'\n",
    "})\n",
    "\n",
    "# Method 3: TSVD\n",
    "print(\"\\n3. TSVD (oracle parameter sweep)...\")\n",
    "k_range = np.arange(1, len(singular_values), max(1, len(singular_values)//50))\n",
    "k_opt, error_tsvd = tsvd_optimal_parameter(A, y_noisy, x_true, k_range)\n",
    "x_tsvd = tsvd_reconstruct(A, y_noisy, k_opt)\n",
    "print(f\"   Optimal k = {k_opt}\")\n",
    "print(f\"   Error: {error_tsvd:.6f}\")\n",
    "results.append({\n",
    "    'method': 'TSVD',\n",
    "    'x_rec': x_tsvd,\n",
    "    'error': error_tsvd,\n",
    "    'parameter': f'k={k_opt}',\n",
    "    'notes': 'Oracle optimal k'\n",
    "})\n",
    "\n",
    "# Method 4: NSIT\n",
    "print(\"\\n4. NSIT + Morozov (automatic)...\")\n",
    "alpha0_nsit = 0.1\n",
    "q_nsit = 0.85\n",
    "x_nsit, hist_nsit = nsit_reconstruct(\n",
    "    A, y_noisy, alpha0_nsit, q_nsit, max_iter=100, \n",
    "    tau=1.05, delta=noise_level, x_true=x_true\n",
    ")\n",
    "error_nsit = np.linalg.norm(x_nsit - x_true) / np.linalg.norm(x_true)\n",
    "print(f\"   Final iteration: {hist_nsit['final_iteration']}\")\n",
    "print(f\"   Error: {error_nsit:.6f}\")\n",
    "results.append({\n",
    "    'method': 'NSIT',\n",
    "    'x_rec': x_nsit,\n",
    "    'error': error_nsit,\n",
    "    'parameter': f'n={hist_nsit[\"final_iteration\"]}',\n",
    "    'notes': 'Automatic (Morozov)'\n",
    "})\n",
    "\n",
    "# Method 5: FNSIT\n",
    "print(\"\\n5. FNSIT + Morozov (automatic, fast)...\")\n",
    "alpha0_fnsit = 0.1\n",
    "q_fnsit = 0.85\n",
    "x_fnsit, hist_fnsit = fnsit_reconstruct(\n",
    "    A, y_noisy, alpha0_fnsit, q_fnsit, max_iter=100,\n",
    "    tau=1.05, delta=noise_level, \n",
    "    inner_steps=8, step_size=0.15,\n",
    "    x_true=x_true\n",
    ")\n",
    "error_fnsit = np.linalg.norm(x_fnsit - x_true) / np.linalg.norm(x_true)\n",
    "print(f\"   Final iteration: {hist_fnsit['final_iteration']}\")\n",
    "print(f\"   Error: {error_fnsit:.6f}\")\n",
    "results.append({\n",
    "    'method': 'FNSIT',\n",
    "    'x_rec': x_fnsit,\n",
    "    'error': error_fnsit,\n",
    "    'parameter': f'n={hist_fnsit[\"final_iteration\"]}',\n",
    "    'notes': 'Automatic + Fast'\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ All methods completed\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805f6ee",
   "metadata": {},
   "source": [
    "### Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf357786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<20} {'Parameter':<20} {'Rel. Error':<15} {'Notes':<25}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['method']:<20} {r['parameter']:<20} {r['error']:<15.6f} {r['notes']:<25}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ranking\n",
    "sorted_results = sorted(results, key=lambda x: x['error'])\n",
    "print(\"\\nðŸ† RANKING (Best â†’ Worst):\")\n",
    "print(\"-\"*50)\n",
    "medals = ['ðŸ¥‡', 'ðŸ¥ˆ', 'ðŸ¥‰', '4.', '5.']\n",
    "for i, r in enumerate(sorted_results):\n",
    "    print(f\"{medals[i]} {r['method']:<20} Error: {r['error']:.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best method\n",
    "best_method = sorted_results[0]['method']\n",
    "print(f\"\\nâœ¨ WINNER: {best_method} âœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494c956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04abe67",
   "metadata": {},
   "source": [
    "### Visualization 1: All Reconstructions Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9388fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Reconstruction Comparison: All 5 Methods', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 0: True signal and measurements\n",
    "axes[0].plot(t, x_true, 'k-', linewidth=3, label='True Signal', zorder=3)\n",
    "axes[0].plot(t, y_noisy/np.max(np.abs(y_noisy))*np.max(np.abs(x_true)), \n",
    "             'r.', markersize=2, alpha=0.3, label='Noisy Data (scaled)')\n",
    "axes[0].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('t')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_facecolor('#f9f9f9')\n",
    "\n",
    "# Plot each method\n",
    "colors = ['red', 'blue', 'orange', 'purple', 'green']\n",
    "for idx, (r, color) in enumerate(zip(results, colors)):\n",
    "    ax = axes[idx + 1]\n",
    "    \n",
    "    # Plot true and reconstruction\n",
    "    ax.plot(t, x_true, 'k-', linewidth=2, label='True', alpha=0.6, zorder=2)\n",
    "    ax.plot(t, r['x_rec'], color=color, linestyle='--', linewidth=2.5, \n",
    "            label='Reconstruction', alpha=0.9, zorder=3)\n",
    "    ax.fill_between(t, x_true, r['x_rec'], alpha=0.15, color=color, zorder=1)\n",
    "    \n",
    "    # Highlight best method\n",
    "    if r['method'] == best_method:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('gold')\n",
    "            spine.set_linewidth(4)\n",
    "        title_suffix = ' â­ BEST'\n",
    "    else:\n",
    "        title_suffix = ''\n",
    "    \n",
    "    ax.set_title(f\"{r['method']}{title_suffix}\\nError: {r['error']:.5f}, {r['parameter']}\", \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Reconstruction comparison plot complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8915e56",
   "metadata": {},
   "source": [
    "### Visualization 2: Direct Side-by-Side Comparison (All in One Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "\n",
    "# True signal (thick black)\n",
    "ax.plot(t, x_true, 'k-', linewidth=4, label='True Signal', zorder=10, alpha=0.8)\n",
    "\n",
    "# All reconstructions\n",
    "linestyles = [':', '-.', '--', '-', '-']\n",
    "linewidths = [2, 2, 2, 2.5, 3]\n",
    "alphas = [0.7, 0.7, 0.7, 0.8, 0.95]\n",
    "\n",
    "for r, color, ls, lw, alpha in zip(results, colors, linestyles, linewidths, alphas):\n",
    "    label = f\"{r['method']} (err={r['error']:.4f})\"\n",
    "    if r['method'] == best_method:\n",
    "        label += ' â­'\n",
    "    ax.plot(t, r['x_rec'], color=color, linestyle=ls, linewidth=lw, \n",
    "            label=label, alpha=alpha)\n",
    "\n",
    "ax.set_title('All Methods Overlay Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('t', fontsize=12)\n",
    "ax.set_ylabel('Signal Amplitude', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=10, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Overlay comparison plot complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc046d96",
   "metadata": {},
   "source": [
    "### Visualization 3: Error Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "method_names = [r['method'] for r in results]\n",
    "errors = [r['error'] for r in results]\n",
    "\n",
    "bars = ax.bar(range(len(method_names)), errors, color=colors, \n",
    "              alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Highlight best\n",
    "best_idx = errors.index(min(errors))\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(4)\n",
    "\n",
    "ax.set_xticks(range(len(method_names)))\n",
    "ax.set_xticklabels(method_names, rotation=20, ha='right', fontsize=11)\n",
    "ax.set_ylabel('Relative Error', fontsize=12)\n",
    "ax.set_title('Reconstruction Error Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y', which='both')\n",
    "\n",
    "# Add value labels\n",
    "for bar, err in zip(bars, errors):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height * 1.2,\n",
    "            f'{err:.5f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Error bar chart complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e15e01",
   "metadata": {},
   "source": [
    "### Visualization 4: Convergence History (NSIT vs FNSIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Error evolution\n",
    "axes[0].semilogy(hist_nsit['iterations'], hist_nsit['errors'], \n",
    "                'o-', color='purple', linewidth=2, markersize=5, label='NSIT')\n",
    "axes[0].semilogy(hist_fnsit['iterations'], hist_fnsit['errors'], \n",
    "                's-', color='green', linewidth=2, markersize=5, label='FNSIT')\n",
    "axes[0].axhline(error_tik, color='blue', linestyle='--', linewidth=2, label='Tikhonov (oracle)', alpha=0.7)\n",
    "axes[0].axhline(error_tsvd, color='orange', linestyle='--', linewidth=2, label='TSVD (oracle)', alpha=0.7)\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Relative Error', fontsize=11)\n",
    "axes[0].set_title('Convergence: Error vs Iteration', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 2: Residual evolution\n",
    "axes[1].semilogy(hist_nsit['iterations'], hist_nsit['residuals'], \n",
    "                'o-', color='purple', linewidth=2, markersize=5, label='NSIT')\n",
    "axes[1].semilogy(hist_fnsit['iterations'], hist_fnsit['residuals'], \n",
    "                's-', color='green', linewidth=2, markersize=5, label='FNSIT')\n",
    "threshold_line = 1.05 * noise_level * np.linalg.norm(y_noisy)\n",
    "axes[1].axhline(threshold_line, color='red', linestyle='--', linewidth=2, label='Morozov threshold', alpha=0.7)\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Residual ||Ax - y||', fontsize=11)\n",
    "axes[1].set_title('Residual vs Iteration', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 3: Alpha schedule\n",
    "axes[2].semilogy(hist_nsit['iterations'], hist_nsit['alphas'], \n",
    "                'o-', color='purple', linewidth=2, markersize=5, label='NSIT')\n",
    "axes[2].semilogy(hist_fnsit['iterations'], hist_fnsit['alphas'], \n",
    "                's-', color='green', linewidth=2, markersize=5, label='FNSIT')\n",
    "axes[2].set_xlabel('Iteration', fontsize=11)\n",
    "axes[2].set_ylabel('Regularization Î±â‚™', fontsize=11)\n",
    "axes[2].set_title('Regularization Schedule', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Convergence analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf77b21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Pointwise Error Analysis: |x_true - x_rec|', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (ax, r, color) in enumerate(zip(axes, results, colors)):\n",
    "    pointwise_error = np.abs(x_true - r['x_rec'])\n",
    "    \n",
    "    ax.plot(t, pointwise_error, color=color, linewidth=2.5, label='|Error|')\n",
    "    ax.fill_between(t, 0, pointwise_error, alpha=0.3, color=color)\n",
    "    \n",
    "    mean_error = np.mean(pointwise_error)\n",
    "    max_error = np.max(pointwise_error)\n",
    "    \n",
    "    ax.axhline(mean_error, color='black', linestyle='--', linewidth=1.5, alpha=0.6, label=f'Mean: {mean_error:.4f}')\n",
    "    \n",
    "    # Highlight best\n",
    "    if r['method'] == best_method:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('gold')\n",
    "            spine.set_linewidth(3)\n",
    "    \n",
    "    ax.set_title(f\"{r['method']}\\nMax Error: {max_error:.4f}, Mean: {mean_error:.4f}\", \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('|x_true - x_rec|')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "# Hide 6th subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Error analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef666280",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "### Key Mathematical Insights\n",
    "\n",
    "1. **Pseudoinverse** fails catastrophically:\n",
    "   - Amplifies noise through small singular values: $x = \\sum_i \\frac{1}{\\sigma_i}\\langle u_i, y\\rangle v_i$\n",
    "   - No regularization mechanism\n",
    "\n",
    "2. **Tikhonov** provides stability:\n",
    "   - Filter factors: $f_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}$ suppress small $\\sigma_i$\n",
    "   - But requires optimal $\\lambda$ (oracle knowledge)\n",
    "\n",
    "3. **TSVD** truncates noise:\n",
    "   - Hard cutoff at rank $k$\n",
    "   - Also requires oracle knowledge of optimal $k$\n",
    "\n",
    "4. **NSIT** adapts automatically:\n",
    "   - Non-stationary regularization: $\\alpha_n = \\alpha_0 q^n$\n",
    "   - Morozov stopping: no parameter tuning!\n",
    "   - Iteratively balances noise suppression and detail recovery\n",
    "\n",
    "5. **FNSIT** is fastest:\n",
    "   - Approximate inner solve via gradient descent\n",
    "   - Maintains NSIT quality with computational speedup\n",
    "   - Additional implicit regularization from approximate solve\n",
    "\n",
    "### Why FNSIT Wins\n",
    "\n",
    "âœ… **Automatic**: No parameter tuning (unlike Tikhonov/TSVD)  \n",
    "âœ… **Adaptive**: Non-stationary schedule balances noise and details  \n",
    "âœ… **Fast**: Approximate solver significantly faster than exact solve  \n",
    "âœ… **Robust**: Morozov stopping prevents overfitting  \n",
    "âœ… **Better quality**: Implicit regularization from approximate solver  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de017f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Filter Factor Visualization\n",
    "\n",
    "Understand how each method filters the SVD spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Compute filter factors\n",
    "# Tikhonov: f_i = Ïƒ_i / (Ïƒ_iÂ² + Î»)\n",
    "filter_tik = singular_values / (singular_values**2 + lambda_opt)\n",
    "\n",
    "# TSVD: f_i = 1/Ïƒ_i if i â‰¤ k, else 0\n",
    "filter_tsvd = np.zeros_like(singular_values)\n",
    "filter_tsvd[:k_opt] = 1.0 / singular_values[:k_opt]\n",
    "\n",
    "# Pseudoinverse: f_i = 1/Ïƒ_i\n",
    "filter_pinv = 1.0 / singular_values\n",
    "\n",
    "# Plot 1: Filter factors vs singular value index\n",
    "axes[0].semilogy(range(1, len(singular_values)+1), filter_pinv, \n",
    "                'o-', color='red', linewidth=2, markersize=4, label='Pseudoinverse', alpha=0.7)\n",
    "axes[0].semilogy(range(1, len(singular_values)+1), filter_tik, \n",
    "                's-', color='blue', linewidth=2, markersize=4, label='Tikhonov', alpha=0.7)\n",
    "axes[0].semilogy(range(1, len(singular_values)+1), filter_tsvd, \n",
    "                '^-', color='orange', linewidth=2, markersize=4, label='TSVD', alpha=0.7)\n",
    "axes[0].axvline(k_opt, color='orange', linestyle='--', linewidth=2, alpha=0.5, label=f'TSVD cutoff k={k_opt}')\n",
    "axes[0].set_xlabel('Singular Value Index i', fontsize=11)\n",
    "axes[0].set_ylabel('Filter Factor f_i', fontsize=11)\n",
    "axes[0].set_title('Filter Factors: How Each Method Treats Singular Values', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 2: Singular values vs filter factors (phase space)\n",
    "axes[1].loglog(singular_values, filter_pinv, \n",
    "              'o-', color='red', linewidth=2, markersize=4, label='Pseudoinverse', alpha=0.7)\n",
    "axes[1].loglog(singular_values, filter_tik, \n",
    "              's-', color='blue', linewidth=2, markersize=4, label='Tikhonov', alpha=0.7)\n",
    "axes[1].loglog(singular_values[filter_tsvd > 0], filter_tsvd[filter_tsvd > 0], \n",
    "              '^-', color='orange', linewidth=2, markersize=4, label='TSVD', alpha=0.7)\n",
    "axes[1].set_xlabel('Singular Value Ïƒáµ¢', fontsize=11)\n",
    "axes[1].set_ylabel('Filter Factor f_i', fontsize=11)\n",
    "axes[1].set_title('Filter Factors vs Singular Values (Log-Log)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Filter factor analysis complete\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Pseudoinverse: f = 1/Ïƒ â†’ explodes for small Ïƒ (noise amplification)\")\n",
    "print(\"  - Tikhonov: f = Ïƒ/(ÏƒÂ²+Î») â†’ smooth damping of small Ïƒ\")\n",
    "print(\"  - TSVD: f = 1/Ïƒ (if iâ‰¤k) or 0 â†’ hard cutoff\")\n",
    "print(\"  - NSIT/FNSIT: adaptive filtering through iteration (not shown - evolves dynamically)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de01f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This comprehensive comparison demonstrates that **FNSIT achieves the best reconstruction quality** while being:\n",
    "- **Fully automatic** (no parameter tuning)\n",
    "- **Computationally efficient** (fast approximate solver)\n",
    "- **Robust** (Morozov stopping prevents overfitting)\n",
    "\n",
    "The mathematical insight: **non-stationary regularization** with **automatic stopping** outperforms classical methods that require oracle parameter selection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
